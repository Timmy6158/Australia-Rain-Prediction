{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Project Description: Predicting Rain Tomorrow in Australia\n",
    "\n",
    "### Introduction:\n",
    "In this final project, you will explore the machine learning techniques by\n",
    "tackling a real-world problem: predicting whether it will rain tomorrow in various\n",
    "locations across Australia. Weather forecasting is a crucial application of machine\n",
    "learning, with far-reaching implications for agriculture, transportation, and public safety.\n",
    "By the end of this project, you will have built and evaluated several classification models,\n",
    "honing your skills in preprocessing, model selection, and interpretation of results.\n",
    "\n",
    "### Dataset Description\n",
    "The dataset provided contains approximately 10 years of daily\n",
    "weather observations from multiple locations across Australia. Each observation includes\n",
    "various features such as temperature, humidity, wind speed, and rainfall. The target\n",
    "variable, RainTomorrow, indicates whether it rained the following day, with a binary\n",
    "classification of \"Yes\" or \"No\". Specifically, if the rainfall for a given day exceeds 1mm,\n",
    "RainTomorrow is labeled as \"Yes\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Packages\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.11.9' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'd:/msys2/ucrt64/bin/python.exe -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from fancyimpute import IterativeImputer\n",
    "from sklearn.impute import SimpleImputer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import BaggingClassifier, AdaBoostClassifier, RandomForestClassifier\n",
    "import xgboost as xgb\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading and Preprocessing:\n",
    "* Load the Aus dataset into a DataFrame.\n",
    "* Standardize the data: Standardize the features by subtracting the mean and dividing by the standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.11.9' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'd:/msys2/ucrt64/bin/python.exe -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# Read the data\n",
    "X_test = pd.read_csv(\"/Users/timnguyen/Desktop/Machine_Learning/Final-Project/weatherAUS_X_test.csv\", delimiter=',')\n",
    "X_train = pd.read_csv(\"/Users/timnguyen/Desktop/Machine_Learning/Final-Project/weatherAUS_X_train.csv\", delimiter=',')\n",
    "y_test = pd.read_csv(\"/Users/timnguyen/Desktop/Machine_Learning/Final-Project/weatherAUS_y_test.csv\", delimiter=',')\n",
    "y_train = pd.read_csv(\"/Users/timnguyen/Desktop/Machine_Learning/Final-Project/weatherAUS_y_train.csv\", delimiter=',')\n",
    "\n",
    "# Drop 'Date' and 'Location' from encoded datasets\n",
    "X_train.drop(columns=['Date', 'Location'], errors='ignore', inplace=True)\n",
    "X_test.drop(columns=['Date', 'Location'], errors='ignore', inplace=True)\n",
    "\n",
    "# Get numerical features and categorical features\n",
    "numerical_features = X_train.select_dtypes(include=['int64', 'float64']).columns\n",
    "categorical_cols = X_train.select_dtypes(include=['object']).columns\n",
    "\n",
    "# Drop missing values from feature datasets\n",
    "X_train_cleaned = X_train.dropna(subset=numerical_features.tolist() + ['RainToday', 'WindGustDir', 'WindDir9am', 'WindDir3pm'])\n",
    "X_test_cleaned = X_test.dropna(subset=numerical_features.tolist() + ['RainToday', 'WindGustDir', 'WindDir9am', 'WindDir3pm'])\n",
    "\n",
    "# Convert 'Yes'/'No' to Binary values in cleaned datasets for 'RainToday'\n",
    "X_train_cleaned['RainToday'] = X_train_cleaned['RainToday'].map({'Yes': 1, 'No': 0})\n",
    "X_test_cleaned['RainToday'] = X_test_cleaned['RainToday'].map({'Yes': 1, 'No': 0})\n",
    "\n",
    "# Apply one-hot encoding to the categorical columns in both training and testing datasets\n",
    "X_train_encoded = pd.get_dummies(X_train_cleaned, columns=categorical_cols)\n",
    "X_test_encoded = pd.get_dummies(X_test_cleaned, columns=categorical_cols)\n",
    "\n",
    "# Ensure both datasets have the same dummy variables, filling with 0 where categories do not match\n",
    "X_train_encoded, X_test_encoded = X_train_encoded.align(X_test_encoded, join='outer', axis=1, fill_value=0)\n",
    "\n",
    "# Align Y_train and Y_test with the indices of cleaned and encoded feature datasets\n",
    "y_train_aligned = y_train.loc[X_train_encoded.index].copy()\n",
    "y_test_aligned = y_test.loc[X_test_encoded.index].copy()\n",
    "\n",
    "# Convert 'Yes'/'No' to Binary values in Y_train_aligned and Y_test_aligned\n",
    "y_train_aligned['RainTomorrow'] = y_train_aligned['RainTomorrow'].map({'Yes': 1, 'No': 0})\n",
    "y_test_aligned['RainTomorrow'] = y_test_aligned['RainTomorrow'].map({'Yes': 1, 'No': 0})\n",
    "\n",
    "# Drop any remaining missing values in the target datasets\n",
    "y_train_aligned.dropna(subset=['RainTomorrow'], inplace=True)\n",
    "y_test_aligned.dropna(subset=['RainTomorrow'], inplace=True)\n",
    "\n",
    "# Assign it back to y_train and y_test for better interpretations\n",
    "y_train = y_train_aligned\n",
    "y_test = y_test_aligned\n",
    "\n",
    "# Convert column vectors to 1D arrays\n",
    "y_train = y_train.values.ravel()\n",
    "y_test = y_test.values.ravel()\n",
    "\n",
    "# Realign the feature datasets to match the new indices of the target datasets\n",
    "X_train_encoded = X_train_encoded.loc[y_train_aligned.index]\n",
    "X_test_encoded = X_test_encoded.loc[y_test_aligned.index]\n",
    "\n",
    "# Get feature names\n",
    "features = X_train_encoded.columns\n",
    "\n",
    "# Standardize the feature datasets\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_encoded)\n",
    "X_test_scaled = scaler.transform(X_test_encoded)\n",
    "\n",
    "# Print shapes of the datasets\n",
    "print(X_train_scaled.shape)\n",
    "print(X_test_scaled.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.11.9' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'd:/msys2/ucrt64/bin/python.exe -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# Exploratory Data Analysis\n",
    "# Visualize key statistics\n",
    "\n",
    "# Calculate correlation matrix\n",
    "#correlation_m = pd.DataFrame(X_train_scaled).corr()\n",
    "#correlation_n = pd.DataFrame(X_test_scaled).corr()\n",
    "\n",
    "# Visualize correlation matrix\n",
    "\n",
    "# Visualize correlation matrix for training set\n",
    "sns.heatmap(correlation_m, annot=True, cmap='coolwarm', square=True)\n",
    "plt.show()\n",
    "\n",
    "# Visualize correlation matrix for test set\n",
    "sns.heatmap(correlation_n, annot=True, cmap='coolwarm', square=True)\n",
    "plt.show()\n",
    "\n",
    "# Visualize all features in a single graph\n",
    "\n",
    "# Define the batch size (number of features to plot at once)\n",
    "subset_size = 20\n",
    "\n",
    "# Get the total number of features\n",
    "num_features = X_train_scaled.shape[1]\n",
    "\n",
    "# Loop through the features in batches\n",
    "for i in range(0, num_features, subset_size):\n",
    "    \n",
    "    # Handle remaining features (if any)\n",
    "\tif i + subset_size >= num_features:\n",
    "\t\tremaining_features = X_train_scaled[:, i:]\n",
    "\t\tremaining_names = X_train_encoded.columns[i:].tolist()\n",
    "\t\tsns.boxplot(data=remaining_features)\n",
    "\t\tplt.xticks(range(len(remaining_names)), remaining_names, rotation=90)\n",
    "\t\tplt.show()\n",
    "\t\tbreak\n",
    "        \n",
    "\t# Get the current batch of features\n",
    "\tfeatures_to_plot = X_train_scaled[:, i:i+subset_size]\n",
    "\t\n",
    "\t# Get the corresponding feature names\n",
    "\tfeature_names = X_train_encoded.columns[i:i+subset_size].tolist()\n",
    "\t\n",
    "\t# Create a boxplot for each feature\n",
    "\tsns.boxplot(data=features_to_plot)\n",
    "\t\n",
    "\t# Set x-tick labels as feature names\n",
    "\tplt.xticks(range(subset_size), feature_names, rotation=90)\n",
    "\t\n",
    "\t# Show the plot\n",
    "\tplt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensionality Reduction: PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.11.9' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'd:/msys2/ucrt64/bin/python.exe -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# Dimensionality Reduction using PCA\n",
    "# Apply PCA\n",
    "\"\"\"Initialize PCA with a default value which is the total number of components\"\"\"\n",
    "pca = PCA()\n",
    "std_pca = pca.fit_transform(X_train_scaled)\n",
    "\n",
    "# Determine the number of principal components to retain\n",
    "retain = pca.explained_variance_ratio_\n",
    "print(\"List of retained principal components\")\n",
    "print(retain)\n",
    "print(len(retain))\n",
    "print()\n",
    "\n",
    "# Plot explained variance ratio\n",
    "\n",
    "\"\"\"Makes a list of numbers to label the x-axis\"\"\"\n",
    "x_label = []\n",
    "for i in range(retain.size):\n",
    "    x_label.append(i)\n",
    "\n",
    "plt.bar(features, retain)\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()\n",
    "\n",
    "# Choose the number of components based on the explained variance ratio\n",
    "\"\"\"This shows what number of components we need with the threshold of 0.95\"\"\"\n",
    "print(\"Cumulative sum of variance ratios\")\n",
    "Cum_sum_pca = pca.explained_variance_ratio_.cumsum()\n",
    "print(Cum_sum_pca)\n",
    "\n",
    "\"\"\"Variables to find the cumulative sum and the threshold size for 95%\"\"\"\n",
    "cumulative_sum = 0\n",
    "best_comp_size = 0\n",
    "\"\"\"Loops through the list of cumulative values and finds the sum before 95% and records the number of components needed to \n",
    "get that value\"\"\"\n",
    "for sum in Cum_sum_pca:\n",
    "    if sum < 0.95:\n",
    "        cumulative_sum = sum\n",
    "        best_comp_size += 1\n",
    "\n",
    "# Perform PCA with the chosen number of components\n",
    "\"\"\"Create another PCA instance with the best cumulative size\"\"\"\n",
    "new_pca = PCA(n_components=best_comp_size)\n",
    "std_pca = new_pca.fit_transform(X_train_scaled)\n",
    "\n",
    "# Interpretation and Conclusion\n",
    "# Interpret principal components\n",
    "print(\"Explained variance ratio of each principal component:\")\n",
    "\"\"\"Calculates the explained variance ratio again but with a certain number of components\"\"\"\n",
    "retain = new_pca.explained_variance_ratio_\n",
    "print(retain)\n",
    "\n",
    "# Summarize key findings\n",
    "print(\"Summary:\")\n",
    "\"\"\" write your code inside the .format()!\"\"\"\n",
    "print(\"PCA captures {}% of the variance with {} components.\".format(cumulative_sum*100, best_comp_size))\n",
    "\n",
    "# Select the top 90 features (columns) based on the explained variance ratio\n",
    "selected_features = X_train_encoded.columns[np.argsort(retain)[::-1][:90]]\n",
    "print(selected_features)\n",
    "# Drop the rest of the features (columns) from the dataframe\n",
    "X_train_reduced = X_train_encoded[selected_features]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.11.9' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'd:/msys2/ucrt64/bin/python.exe -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Train a decision tree classifier with default parameters\n",
    "DTC = DecisionTreeClassifier()\n",
    "DTC.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Evaluate the classifier on the training set\n",
    "train_set_predict = DTC.predict(X_train_scaled)\n",
    "train_set_accuracy = accuracy_score(y_train, train_set_predict)\n",
    "print(f\"Train Accuracy: {train_set_accuracy}\\n\")\n",
    "\n",
    "# Evaluate the classifier on the testing set\n",
    "test_set_predict = DTC.predict(X_test_scaled)\n",
    "test_set_accuracy = accuracy_score(y_test, test_set_predict)\n",
    "print(f\"Test Accuracy: {test_set_accuracy}\\n\")\n",
    "\n",
    "\n",
    "# Visualize the learned tree\n",
    "\"\"\" use plot_tree from sklearn.tree \"\"\"\n",
    "#plt.figure(figsize=(20,15))\n",
    "#plot_tree(DTC, filled=True)\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayesian "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.11.9' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'd:/msys2/ucrt64/bin/python.exe -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# Train a Naive Bayes classifier\n",
    "NBC_model = GaussianNB()\n",
    "\n",
    "NBC_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "predicted = NBC_model.predict(X_test_scaled)\n",
    "\n",
    "accuracy = accuracy_score(predicted, y_test)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machine (SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.11.9' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'd:/msys2/ucrt64/bin/python.exe -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Comparison with SVM Classifiers: apply linear and a non-linear SVM classifiers\n",
    "linear_svm = LinearSVC()\n",
    "\n",
    "linear_svm.fit(X_train_scaled, y_train)\n",
    "\n",
    "prediction = linear_svm.predict(X_test_scaled)\n",
    "linear_svm_accuracy = accuracy_score(y_test, prediction)\n",
    "print(\"Linear SVM Accuracy:\", linear_svm_accuracy)\n",
    "\n",
    "#HalvingGridSearchCV use this\n",
    "# Task 4: Hyperparameter Tuning : Use GridSearchCV to find the best hyperparameters and print them out\n",
    "#param_grid = {'C': [0.1, 1, 10, 100], 'gamma': [1, 0.1, 0.01, 0.001]}\n",
    "##grid_search = 0 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Logistics Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.11.9' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'd:/msys2/ucrt64/bin/python.exe -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import sklearn.linear_model\n",
    "from planar_utils import plot_decision_boundary, sigmoid\n",
    "\n",
    "# Train the logistic regression classifier\n",
    "clf = sklearn.linear_model.LogisticRegression();\n",
    "\n",
    "print(X_train_scaled.shape, y_train.shape)\n",
    "print(X_test_scaled.shape, y_test.shape)\n",
    "\n",
    "clf.fit(X_train_scaled, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.11.9' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'd:/msys2/ucrt64/bin/python.exe -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# Print accuracy\n",
    "LR_predictions = clf.predict(X_test_scaled)\n",
    "print ('Accuracy of logistic regression: %d ' % float((np.dot(y_test, LR_predictions) + np.dot(1 - y_test,1 - LR_predictions)) / float(y_test.size) * 100) +\n",
    "       '% ' + \"(percentage of correctly labelled datapoints)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble methods: Bagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.11.9' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'd:/msys2/ucrt64/bin/python.exe -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# Define ensemble classifiers\n",
    "classifiers = {\n",
    "    \"Bagging\": BaggingClassifier(estimator=DecisionTreeClassifier(), n_estimators=50, random_state=42),\n",
    "}\n",
    "\n",
    "# Train and evaluate classifiers\n",
    "for name, clf in classifiers.items():\n",
    "    print(f\"Training {name}...\")\n",
    "    #train and predict for each model\n",
    "    clf.fit(X_train_scaled, y_train)\n",
    "\n",
    "    # Predict probabilities for the test data\n",
    "    prediction = clf.predict(X_test_scaled)\n",
    "    y_pred_prob = clf.predict_proba(X_test_scaled)[:, 1]\n",
    "    #y_pred_class = np.where(y_pred_prob >= 0.5, 1, 0)\n",
    "        \n",
    "    # Compute evaluation metrics\n",
    "    confuse_matrix = confusion_matrix(y_test, prediction)\n",
    "\n",
    "    print(confuse_matrix)\n",
    "\n",
    "    class_report = classification_report(y_test, prediction)\n",
    "\n",
    "    print(class_report)\n",
    "\n",
    "    # Compute ROC curve\n",
    "\n",
    "    #Calculates fpr and tpr\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)\n",
    "\n",
    "    #Computes area under the curve\n",
    "    roc_auc = auc(fpr,tpr)\n",
    "\n",
    "    # Plot ROC curve\n",
    "    plt.figure()\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('{} (ROC) Curve'.format(name))\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is the Pre-processing data without encoding categorical values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.11.9' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'd:/msys2/ucrt64/bin/python.exe -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# Read the data\n",
    "X_test = pd.read_csv(\"/Users/timnguyen/Desktop/Machine_Learning/Final-Project/weatherAUS_X_test.csv\", delimiter=',')\n",
    "X_train = pd.read_csv(\"/Users/timnguyen/Desktop/Machine_Learning/Final-Project/weatherAUS_X_train.csv\", delimiter=',')\n",
    "y_test = pd.read_csv(\"/Users/timnguyen/Desktop/Machine_Learning/Final-Project/weatherAUS_y_test.csv\", delimiter=',')\n",
    "y_train = pd.read_csv(\"/Users/timnguyen/Desktop/Machine_Learning/Final-Project/weatherAUS_y_train.csv\", delimiter=',')\n",
    "\n",
    "\n",
    "# Replace 'Yes' and 'No' with 1 and 0\n",
    "y_train['RainTomorrow'] = y_train['RainTomorrow'].map({'Yes': 1, 'No': 0})\n",
    "y_test['RainTomorrow'] = y_test['RainTomorrow'].map({'Yes': 1, 'No': 0})\n",
    "\n",
    "# Drop rows with missing values in y_train and get the indices of remaining rows\n",
    "y_train = y_train.dropna()\n",
    "train_indices = y_train.index\n",
    "\n",
    "# Select rows in X_train with the same indices\n",
    "X_train_select = X_train.iloc[train_indices]\n",
    "\n",
    "# Repeat for the test set\n",
    "y_test = y_test.dropna()\n",
    "test_indices = y_test.index\n",
    "X_test_select = X_test.loc[test_indices]\n",
    "\n",
    "#Drop date column\n",
    "X_train_select.drop(columns=['Date'], axis=1, inplace=True)\n",
    "X_test_select.drop(columns=['Date'], axis=1, inplace=True)\n",
    "\n",
    "#Drop categorical columns for now\n",
    "X_train_select.drop(columns=['Location', 'WindGustDir', 'WindDir9am', 'WindDir3pm'],axis=1, inplace=True)\n",
    "X_test_select.drop(columns=['Location', 'WindGustDir', 'WindDir9am', 'WindDir3pm'],axis=1, inplace=True)\n",
    "\n",
    "#Drop features with no strong correlation with other features\n",
    "X_train_select.drop(columns=['Rainfall', 'Sunshine', 'Humidity9am', 'Humidity3pm', 'Cloud9am', 'Cloud3pm', 'RainToday'],axis=1, inplace=True)\n",
    "X_test_select.drop(columns=['Rainfall', 'Sunshine', 'Humidity9am', 'Humidity3pm', 'Cloud9am', 'Cloud3pm', 'RainToday'],axis=1, inplace=True)\n",
    "\n",
    "# Drop columns that are not in the selected features\n",
    "#X_train = X_train[selected_features]\n",
    "#X_test = X_test[selected_features]\n",
    "\n",
    "features = X_train_select.columns\n",
    "\n",
    "# Checks to see if there are the same number of rows\n",
    "\n",
    "# Convert column vectors to 1D arrays\n",
    "y_train = y_train.values.ravel()\n",
    "y_test = y_test.values.ravel()\n",
    "\n",
    "\"\"\"# Impute missing values in categorical variables\n",
    "cat_imputer = SimpleImputer(strategy='most_frequent')\n",
    "\n",
    "# Select categorical columns\n",
    "cat_cols = X_train.select_dtypes(include=['object']).columns\n",
    "\n",
    "X_train[cat_cols] = cat_imputer.fit_transform(X_train[cat_cols])\n",
    "X_test[cat_cols] = cat_imputer.transform(X_test[cat_cols])\n",
    "\n",
    "# One-Hot Encoding\n",
    "X_train = pd.get_dummies(X_train)\n",
    "X_test = pd.get_dummies(X_test)\n",
    "\n",
    "# Make sure both training and test dataframes have the same columns\n",
    "X_train, X_test = X_train.align(X_test, join='left', axis=1)\"\"\"\n",
    "\n",
    "# Impute missing values in numerical variables\n",
    "num_imputer = SimpleImputer(strategy='mean')\n",
    "\n",
    "# Select numerical columns\n",
    "#num_cols = X_train.select_dtypes(include=[np.number]).columns\n",
    "num_imputer.fit(X_train_select)\n",
    "\n",
    "X_train_imputed = num_imputer.transform(X_train_select)\n",
    "X_test_imputed = num_imputer.transform(X_test_select)\n",
    "\n",
    "X_train_df = pd.DataFrame(X_train_imputed, columns=features)\n",
    "X_test_df = pd.DataFrame(X_test_imputed, columns=features)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_df)\n",
    "X_test_scaled = scaler.transform(X_test_df)\n",
    "\n",
    "features = X_train_select.columns\n",
    "\n",
    "print(X_train_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is the Data pre-processing with encoding categorical values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.11.9' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'd:/msys2/ucrt64/bin/python.exe -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# Read the data\n",
    "X_test = pd.read_csv(\"/Users/timnguyen/Desktop/Machine_Learning/Final-Project/weatherAUS_X_test.csv\", delimiter=',').head(20000)\n",
    "X_train = pd.read_csv(\"/Users/timnguyen/Desktop/Machine_Learning/Final-Project/weatherAUS_X_train.csv\", delimiter=',').head(1000)\n",
    "y_test = pd.read_csv(\"/Users/timnguyen/Desktop/Machine_Learning/Final-Project/weatherAUS_y_test.csv\", delimiter=',').head(20000)\n",
    "y_train = pd.read_csv(\"/Users/timnguyen/Desktop/Machine_Learning/Final-Project/weatherAUS_y_train.csv\", delimiter=',').head(1000)\n",
    "\n",
    "#Drop 'Date' and 'Location' from encoded datasets if they are not needed\n",
    "X_train.drop(columns=['Date', 'Location'], errors='ignore', inplace=True)\n",
    "X_test.drop(columns=['Date', 'Location'], errors='ignore', inplace=True)\n",
    "\n",
    "numerical_features = X_train.select_dtypes(include=['int64', 'float64']).columns\n",
    "categorical_cols = X_train.select_dtypes(include=['object']).columns\n",
    "\n",
    "#Drop missing values from feature datasets\n",
    "X_train_cleaned = X_train.dropna(subset=numerical_features.tolist() + ['RainToday', 'WindGustDir', 'WindDir9am', 'WindDir3pm'])\n",
    "X_test_cleaned = X_test.dropna(subset=numerical_features.tolist() + ['RainToday', 'WindGustDir', 'WindDir9am', 'WindDir3pm'])\n",
    "\n",
    "#Convert 'Yes'/'No' to Binary values in cleaned datasets for 'RainToday'\n",
    "X_train_cleaned['RainToday'] = X_train_cleaned['RainToday'].map({'Yes': 1, 'No': 0})\n",
    "X_test_cleaned['RainToday'] = X_test_cleaned['RainToday'].map({'Yes': 1, 'No': 0})\n",
    "\n",
    "#Apply one-hot encoding to the categorical columns in both training and testing datasets\n",
    "X_train_encoded = pd.get_dummies(X_train_cleaned, columns=categorical_cols)\n",
    "X_test_encoded = pd.get_dummies(X_test_cleaned, columns=categorical_cols)\n",
    "\n",
    "#Ensure both datasets have the same dummy variables, filling with 0 where categories do not match\n",
    "X_train_encoded, X_test_encoded = X_train_encoded.align(X_test_encoded, join='outer', axis=1, fill_value=0)\n",
    "\n",
    "#Align Y_train and Y_test with the indices of cleaned and encoded feature datasets\n",
    "y_train_aligned = y_train.loc[X_train_encoded.index].copy()\n",
    "y_test_aligned = y_test.loc[X_test_encoded.index].copy()\n",
    "\n",
    "#Convert 'Yes'/'No' to Binary values in Y_train_aligned and Y_test_aligned\n",
    "y_train_aligned['RainTomorrow'] = y_train_aligned['RainTomorrow'].map({'Yes': 1, 'No': 0})\n",
    "y_test_aligned['RainTomorrow'] = y_test_aligned['RainTomorrow'].map({'Yes': 1, 'No': 0})\n",
    "\n",
    "#Drop any remaining missing values in the target datasets\n",
    "y_train_aligned.dropna(subset=['RainTomorrow'], inplace=True)\n",
    "y_test_aligned.dropna(subset=['RainTomorrow'], inplace=True)\n",
    "\n",
    "y_train = y_train_aligned\n",
    "y_test = y_test_aligned\n",
    "\n",
    "# Convert column vectors to 1D arrays\n",
    "y_train = y_train.values.ravel()\n",
    "y_test = y_test.values.ravel()\n",
    "\n",
    "#Realign the feature datasets to match the new indices of the target datasets\n",
    "X_train_encoded = X_train_encoded.loc[y_train_aligned.index]\n",
    "X_test_encoded = X_test_encoded.loc[y_test_aligned.index]\n",
    "\n",
    "#Get feature names\n",
    "features = X_train_encoded.columns\n",
    "\n",
    "#Standardize the feature datasets\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_encoded)\n",
    "X_test_scaled = scaler.transform(X_test_encoded)\n",
    "\n",
    "#Print shapes of the datasets\n",
    "print(X_train_scaled.shape)\n",
    "print(X_test_scaled.shape)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
